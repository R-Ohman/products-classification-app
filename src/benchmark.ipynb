{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133cfe37-d824-427a-ba60-3dfe01d6ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e64ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/original_model.tflite'\n",
    "CLASS_NAMES = sorted(os.listdir('../dataset/products/validation')) # directory names are the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd6521-993c-41e2-9631-dcec13b60a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filename):\n",
    "    image = tf.keras.utils.load_img(filename, target_size=(224, 224, 3))\n",
    "    image = tf.keras.utils.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ec8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(MODEL_PATH)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbb0d7",
   "metadata": {},
   "source": [
    "Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f385321f-bfe8-4997-9619-ec6f363113f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_PATH = 'test.jpg'\n",
    "image = load_and_preprocess_image(TEST_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90167242-28e5-4696-af67-e944a4ceac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff76a7c5-f2ec-4125-b8fd-c81534595d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e0936-bc66-41d2-9f30-b06c2aa778d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.8 ms ± 70 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c0bba77-275d-45e0-be0c-607c8105286c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6806073e-11, 1.1282087e-12, 5.1287110e-08, 1.1686058e-06,\n",
       "        6.3463885e-16, 6.8896993e-13, 1.2214249e-06, 5.1749035e-12,\n",
       "        4.5067642e-13, 3.9826781e-17, 2.5354030e-11, 3.8585881e-11,\n",
       "        3.9000976e-11, 1.5328704e-12, 9.9999750e-01]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171a68d4-d856-4ca5-9b88-fc74ab557e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomato  prediction:  0.9999975\n"
     ]
    }
   ],
   "source": [
    "print(CLASS_NAMES[np.argmax(predictions)], \" prediction: \" ,np.max(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606f5cc",
   "metadata": {},
   "source": [
    "Many images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff2a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4caa9",
   "metadata": {},
   "source": [
    "Prepare generator, that returns tuple (images, classes). Image shape is (batch_size x pixels x pixels x three_colors). Classes shape is (batch_size x classes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2145992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '../dataset/products/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc056592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  100\n",
      "avg time:  25.68945082107393 ms\n",
      "accuracy:  0.9421994559640713\n",
      "\n",
      "iteration:  200\n",
      "avg time:  25.719411337553566 ms\n",
      "accuracy:  0.9594122923686929\n",
      "\n",
      "iteration:  300\n",
      "avg time:  25.7530584683846 ms\n",
      "accuracy:  0.9593759604631747\n",
      "\n",
      "iteration:  400\n",
      "avg time:  25.788022990238638 ms\n",
      "accuracy:  0.9603865603603265\n",
      "\n",
      "iteration:  500\n",
      "avg time:  25.87743481238207 ms\n",
      "accuracy:  0.959666139515139\n",
      "\n",
      "iteration:  600\n",
      "avg time:  25.995644475775034 ms\n",
      "accuracy:  0.9631133746887699\n",
      "\n",
      "iteration:  700\n",
      "avg time:  26.00040177305823 ms\n",
      "accuracy:  0.9619821793583851\n",
      "\n",
      "iteration:  800\n",
      "avg time:  25.99955557586251 ms\n",
      "accuracy:  0.9635474601463829\n",
      "\n",
      "iteration:  900\n",
      "avg time:  26.01253417435285 ms\n",
      "accuracy:  0.9626253277719484\n",
      "\n",
      "iteration:  1000\n",
      "avg time:  25.984514247882853 ms\n",
      "accuracy:  0.9597899847975085\n",
      "\n",
      "Total time: 26.010498762130737 s\n"
     ]
    }
   ],
   "source": [
    "val_matrix = np.zeros((15, 15))\n",
    "total_time = 0\n",
    "TEST_LIMIT = 1000\n",
    "\n",
    "for iteration, (image, class_) in enumerate(test_generator):\n",
    "    class_idx = np.argmax(class_)\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    interpreter.invoke()\n",
    "    total_time += time.time() - start_time\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "    val_matrix[class_idx] += predictions.flatten()\n",
    "    \n",
    "    if (iteration > 0 and iteration % 100 == 0):\n",
    "        print(\"iteration: \", iteration)\n",
    "        print(\"avg time: \", 1000 * total_time / (iteration + 1), \"ms\")\n",
    "        print(\"accuracy: \", np.sum(np.diag(val_matrix)/np.sum(val_matrix)))\n",
    "        print()\n",
    "\n",
    "    if iteration >= TEST_LIMIT:\n",
    "        break\n",
    "\n",
    "print(\"Total time:\", total_time, \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
